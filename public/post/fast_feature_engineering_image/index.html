<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=generator content="Source Themes Academia 4.3.1">
<meta name=generator content="Hugo 0.88.1">
<meta name=author content="Sayar Banerjee">
<meta name=description content="Make your images more suitable to feed into ML systems">
<link rel=alternate hreflang=en-us href=https://examplesite.org/post/fast_feature_engineering_image/>
<meta name=theme-color content="#fc6f5c">
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous>
<link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap">
<link rel=stylesheet href=/css/academia.min.76e69f7370e1e1222344c0109d841c44.css>
<link rel=manifest href=/site.webmanifest>
<link rel=icon type=image/png href=/img/icon.png>
<link rel=apple-touch-icon type=image/png href=/img/icon-192.png>
<link rel=canonical href=https://examplesite.org/post/fast_feature_engineering_image/>
<meta property="twitter:card" content="summary_large_image">
<meta property="og:site_name" content="Portfolio">
<meta property="og:url" content="https://examplesite.org/post/fast_feature_engineering_image/">
<meta property="og:title" content="Fast Feature Engineering in Python; Image Data | Portfolio">
<meta property="og:description" content="Make your images more suitable to feed into ML systems"><meta property="og:image" content="https://examplesite.org/post/fast_feature_engineering_image/featured.jpeg">
<meta property="twitter:image" content="https://examplesite.org/post/fast_feature_engineering_image/featured.jpeg"><meta property="og:locale" content="en-us">
<meta property="article:published_time" content="2021-09-16T00:00:00+00:00">
<meta property="article:modified_time" content="2021-09-16T00:00:00+00:00">
<title>Fast Feature Engineering in Python; Image Data | Portfolio</title>
</head>
<body id=top data-spy=scroll data-target=#TableOfContents data-offset=71>
<aside class=search-results id=search>
<div class=container>
<section class=search-header>
<div class="row no-gutters justify-content-between mb-3">
<div class=col-6>
<h1>Search</h1>
</div>
<div class="col-6 col-search-close">
<a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a>
</div>
</div>
<div id=search-box>
</div>
</section>
<section class=section-search-results>
<div id=search-hits>
</div>
</section>
</div>
</aside>
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id=navbar-main>
<div class=container>
<a class=navbar-brand href=/>Portfolio</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span>
</button>
<div class="collapse navbar-collapse" id=navbar>
<ul class="navbar-nav ml-auto">
<li class=nav-item>
<a class=nav-link href=/#about><span>Home</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#posts><span>Posts</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#projects><span>Projects</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#contact><span>Contact</span></a>
</li>
<li class=nav-item>
<a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a>
</li>
</ul>
</div>
</div>
</nav>
<article class="article py-5" itemscope itemtype=http://schema.org/Article>
<div class="container split-header">
<div class="row justify-content-center">
<div class=col-lg-8>
<img class="img-fluid w-100" src=/post/fast_feature_engineering_image/featured_hu8a696656d06d00dd0aa1666c54428714_251054_680x500_fill_q90_lanczos_smart1.jpeg itemprop=image alt>
<span class=article-header-caption>Image credit: <a href=https://unsplash.com/@jonathanborba><strong>Jonathan Borba</strong></a></span>
</div>
<div class=col-lg-8>
<h1 itemprop=name>Fast Feature Engineering in Python; Image Data</h1>
<p class=page-subtitle>Make your images more suitable to feed into ML systems</p>
<meta content="2021-09-16 00:00:00 +0000 UTC" itemprop=datePublished>
<meta content="2021-09-16 00:00:00 +0000 UTC" itemprop=dateModified>
<div class=article-metadata>
<div>
<span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/admin/>Sayar Banerjee</a></span>
</div>
<span class=article-date>
<time>Sep 16, 2021</time>
</span>
<span class=middot-divider></span>
<span class=article-reading-time>
8 min read
</span>
</div>
</div>
</div>
</div>
</div>
<div class=article-container>
<div class=article-style itemprop=articleBody>
<blockquote>
<p>“Finding patterns is easy in any kind of data-rich environment; that’s what mediocre gamblers do. The key is in determining whether the patterns represent noise or signal.”<br>
― <strong>Nate Silver</strong></p>
</blockquote>
<p>This article is part 2 of my “Fast Feature Engineering” series. If you have not read my first article which talks about tabular data, then I request you to check it out here:</p>
<p><a href=https://towardsdatascience.com/fast-feature-engineering-in-python-tabular-data-d050b68bb178 title=https://towardsdatascience.com/fast-feature-engineering-in-python-tabular-data-d050b68bb178><strong>Fast Feature Engineering in Python: Tabular Data</strong></a><a href=https://towardsdatascience.com/fast-feature-engineering-in-python-tabular-data-d050b68bb178></a></p>
<p>This article will look at some of the best practices to follow when performing image processing as part of our machine learning workflow.</p>
<hr>
<h3 id=libraries>Libraries</h3>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#f92672>import</span> random  
<span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image  
<span style=color:#f92672>import</span> cv2  
<span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np  
<span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> pyplot <span style=color:#66d9ef>as</span> plt  
<span style=color:#f92672>import</span> json  
<span style=color:#f92672>import</span> albumentations <span style=color:#66d9ef>as</span> A  
<span style=color:#f92672>import</span> torch  
<span style=color:#f92672>import</span> torchvision.models <span style=color:#66d9ef>as</span> models  
<span style=color:#f92672>import</span> torchvision.transforms <span style=color:#66d9ef>as</span> transforms  
<span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn  
<span style=color:#f92672>from</span> tqdm <span style=color:#f92672>import</span> tqdm_notebook  
<span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> DataLoader  
<span style=color:#f92672>from</span> torchvision.datasets <span style=color:#f92672>import</span> CIFAR10
</code></pre></div><hr>
<h3 id=resizescale-images>Resize/Scale Images</h3>
<p>Resizing is the most fundamental transformation done by deep learning practitioners in the field. The primary reason for doing this is to ensure that the input received by our deep learning system is <strong>consistent</strong>.</p>
<p>Another reason for resizing is to <strong>reduce the number of parameters</strong> in the model. Smaller dimensions signify a smaller neural network and hence, saves us the time and computation power required to train our model.</p>
<h4 id=_what-about-the-loss-of-information_><strong><em>What about the loss of information?</em></strong></h4>
<p>Some information is indeed <strong>lost</strong> when you resize down from a larger image. However, depending on your task, you can choose how much information you’re willing to sacrifice for training time and compute resources.</p>
<p>For example, an <a href=https://en.wikipedia.org/wiki/Object_detection><strong>object detection task</strong></a> will require you to maintain the image&rsquo;s aspect ratio since the goal is to detect the exact position of objects.</p>
<p>In contrast, an image classification task may require you to resize all images down to a specified size (224 x 224 is a good rule of thumb).</p>
<p><img src=/posts_img/fast_feature_engineering/img_1.jpeg alt></p>
<p>After resizing our image looks like this:</p>
<p><img src=/posts_img/fast_feature_engineering/img_2.jpeg alt></p>
<h4 id=_why-perform-imagescaling_><em>Why perform image scaling?</em></h4>
<p>Similar to tabular data, scaling images for classification tasks can help our deep learning model&rsquo;s learning rate to converge to the minima better.</p>
<p>Scaling ensures that a particular dimension does not dominate others. I found a fantastic answer on StackExchange regarding this. You can read it <a href=https://stats.stackexchange.com/questions/185853/why-do-we-need-to-normalize-the-images-before-we-put-them-into-cnn><strong>here</strong></a>.</p>
<p>One type of feature scaling is the process of <strong>standardizing</strong> our pixel values. We do this by subtracting the mean of each channel from its pixel value and then divide it via standard deviation.</p>
<p>This is a popular choice of feature engineering when training models for classification tasks.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>mean <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(img_resized, axis<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>), keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
std <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>std(img_resized, axis<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>), keepdims<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
img_std <span style=color:#f92672>=</span> (img_resized <span style=color:#f92672>-</span> mean) <span style=color:#f92672>/</span> std
</code></pre></div><p><strong><em>Note: Like resizing, one may not want to do image scaling when performing object detection and image generation tasks.</em></strong></p>
<p>The example code above demonstrates the process of scaling an image via standardization. There are other forms of scaling such as <strong>centering</strong> and <strong>normalization</strong>.</p>
<hr>
<h3 id=augmentations-classification>Augmentations (Classification)</h3>
<p>The primary motivation behind augmenting images is due to the appreciable data requirement for computer vision tasks. Often, obtaining enough images for training can prove to be challenging for a multitude of reasons.</p>
<p>Image augmentation enables us to create new training samples by slightly modifying the original ones.</p>
<p>In this example, we will look at how to apply vanilla augmentations for a classification task. We can use the out of the box implementations of the <strong>Albumentations</strong> library to do this:</p>
<p><img src=/posts_img/fast_feature_engineering/img_3.jpeg alt>
<img src=/posts_img/fast_feature_engineering/img_4.jpeg alt>
<img src=/posts_img/fast_feature_engineering/img_5.jpeg alt></p>
<p>By applying image augmentations, our deep learning models can generalize better to the task (avoid overfitting), thereby increasing its predictive power on unseen data.</p>
<hr>
<h3 id=augmentations-object-detection>Augmentations (Object Detection)</h3>
<p>The Albumentations library can also be used to create augmentations for other tasks such as object detections. Object detection requires us to create bounding boxes around the object of interest.</p>
<p>Working with raw data can prove to be challenging when trying to annotate images with the coordinates for the bounding boxes.</p>
<p>Fortunately, there are many publicly and freely available datasets that we can use to create an augmentation pipeline for object detection. One such dataset is the <a href=https://public.roboflow.com/object-detection/chess-full><strong>Chess Dataset</strong></a>.</p>
<p>The dataset contains 606 images of chess pieces on a chessboard.</p>
<p>Along with the images, a JSON file is provided that contains all the information pertaining to the bounding boxes for each chess piece in a single image.</p>
<p>By writing a simple function, we can visualize the data after the augmentation is applied:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;_annotations.coco.json&#34;</span>) <span style=color:#66d9ef>as</span> f:
    json_file <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>load(f)
    
x_min, y_min, w, h <span style=color:#f92672>=</span> json_file[<span style=color:#e6db74>&#39;annotations&#39;</span>][<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;bbox&#39;</span>]
x_min, x_max, y_min, y_max <span style=color:#f92672>=</span> int(x_min), int(x_min <span style=color:#f92672>+</span> w), int(y_min), int(y_min <span style=color:#f92672>+</span> h)

<span style=color:#66d9ef>def</span> <span style=color:#a6e22e>visualize_bbox</span>(img, bbox, class_name, color<span style=color:#f92672>=</span>(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>255</span>, <span style=color:#ae81ff>0</span>), thickness<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>):
    x_min, y_min, w, h <span style=color:#f92672>=</span> bbox
    x_min, x_max, y_min, y_max <span style=color:#f92672>=</span> int(x_min), int(x_min <span style=color:#f92672>+</span> w), int(y_min), int(y_min <span style=color:#f92672>+</span> h)

    cv2<span style=color:#f92672>.</span>rectangle(img, (x_min, y_min), (x_max, y_max), color<span style=color:#f92672>=</span>color, thickness<span style=color:#f92672>=</span>thickness)

    ((text_width, text_height), _) <span style=color:#f92672>=</span> cv2<span style=color:#f92672>.</span>getTextSize(class_name, cv2<span style=color:#f92672>.</span>FONT_HERSHEY_SIMPLEX, <span style=color:#ae81ff>0.35</span>, <span style=color:#ae81ff>1</span>)    
    cv2<span style=color:#f92672>.</span>rectangle(img, (x_min, y_min <span style=color:#f92672>-</span> int(<span style=color:#ae81ff>1.3</span> <span style=color:#f92672>*</span> text_height)), (x_min <span style=color:#f92672>+</span> text_width, y_min), BOX_COLOR, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
    cv2<span style=color:#f92672>.</span>putText(
        img,
        text<span style=color:#f92672>=</span>class_name,
        org<span style=color:#f92672>=</span>(x_min, y_min <span style=color:#f92672>-</span> int(<span style=color:#ae81ff>0.3</span> <span style=color:#f92672>*</span> text_height)),
        fontFace<span style=color:#f92672>=</span>cv2<span style=color:#f92672>.</span>FONT_HERSHEY_SIMPLEX,
        fontScale<span style=color:#f92672>=</span><span style=color:#ae81ff>0.35</span>, 
        color<span style=color:#f92672>=</span>(<span style=color:#ae81ff>255</span>, <span style=color:#ae81ff>255</span>, <span style=color:#ae81ff>255</span>), 
        lineType<span style=color:#f92672>=</span>cv2<span style=color:#f92672>.</span>LINE_AA,
    )
    <span style=color:#66d9ef>return</span> img
  
bbox_img <span style=color:#f92672>=</span> visualize_bbox(np<span style=color:#f92672>.</span>array(img), 
                          json_file[<span style=color:#e6db74>&#39;annotations&#39;</span>][<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;bbox&#39;</span>], 
                          class_name<span style=color:#f92672>=</span>json_file[<span style=color:#e6db74>&#39;categories&#39;</span>][<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;name&#39;</span>])

Image<span style=color:#f92672>.</span>fromarray(bbox_img)
</code></pre></div><p><img src=/Users/Banner/Downloads/medium-export-aa5b5fa1b4f15ba326f851375de5c386499a5652f183eac85ab56b6ca8924b20/posts/md_1638090489769/img/1__MFakz3EYf73afrl__aT3S2A.jpeg alt>
<img src=/posts_img/fast_feature_engineering/img_6.jpeg alt></p>
<p>Now, let’s try to create an augmentation pipeline using Albumentations.</p>
<p>The JSON file that contains the annotation information has the following keys:</p>
<p><code>dict_keys([‘info’, ‘licenses’, ‘categories’, ‘images’, ‘annotations’])</code></p>
<p><code>images</code> contains information about the image file whereas <code>annotations</code> contains information about the bounding boxes for each object in an image.</p>
<p>Finally, <code>categories</code> contains keys that map to the type of chess pieces in the image.</p>
<pre tabindex=0><code>image_list = json_file.get('images')  
anno_list = json_file.get('annotations')  
cat_list = json_file.get('categories')
</code></pre><p><code>image_list</code> :</p>
<pre tabindex=0><code>[{'id': 0,  
  'license': 1,  
  'file_name': 'IMG_0317_JPG.rf.00207d2fe8c0a0f20715333d49d22b4f.jpg',  
  'height': 416,  
  'width': 416,  
  'date_captured': '2021-02-23T17:32:58+00:00'},  
 {'id': 1,  
  'license': 1,  
  'file_name': '5a8433ec79c881f84ef19a07dc73665d_jpg.rf.00544a8110f323e0d7721b3acf2a9e1e.jpg',  
  'height': 416,  
  'width': 416,  
  'date_captured': '2021-02-23T17:32:58+00:00'},  
 {'id': 2,  
  'license': 1,  
  'file_name': '675619f2c8078824cfd182cec2eeba95_jpg.rf.0130e3c26b1bf275bf240894ba73ed7c.jpg',  
  'height': 416,  
  'width': 416,  
  'date_captured': '2021-02-23T17:32:58+00:00'},  
.  
.  
.  
.
</code></pre><p><code>anno_list</code> :</p>
<pre tabindex=0><code>[{'id': 0,  
  'image_id': 0,  
  'category_id': 7,  
  'bbox': [220, 14, 18, 46.023746508293286],  
  'area': 828.4274371492792,  
  'segmentation':],  
  'iscrowd': 0},  
 {'id': 1,  
  'image_id': 1,  
  'category_id': 8,  
  'bbox': [187, 103, 22.686527154676014, 59.127992255841036],  
  'area': 1341.4088019136107,  
  'segmentation': [],  
  'iscrowd': 0},  
 {'id': 2,  
  'image_id': 2,  
  'category_id': 10,  
  'bbox': [203, 24, 24.26037020843023, 60.5],  
  'area': 1467.752397610029,  
  'segmentation': [],  
  'iscrowd': 0},  
.  
.  
.  
.
</code></pre><p><code>cat_list</code> :</p>
<pre tabindex=0><code>[{'id': 0, 'name': 'pieces', 'supercategory': 'none'},  
 {'id': 1, 'name': 'bishop', 'supercategory': 'pieces'},  
 {'id': 2, 'name': 'black-bishop', 'supercategory': 'pieces'},  
 {'id': 3, 'name': 'black-king', 'supercategory': 'pieces'},  
 {'id': 4, 'name': 'black-knight', 'supercategory': 'pieces'},  
 {'id': 5, 'name': 'black-pawn', 'supercategory': 'pieces'},  
 {'id': 6, 'name': 'black-queen', 'supercategory': 'pieces'},  
 {'id': 7, 'name': 'black-rook', 'supercategory': 'pieces'},  
 {'id': 8, 'name': 'white-bishop', 'supercategory': 'pieces'},  
 {'id': 9, 'name': 'white-king', 'supercategory': 'pieces'},  
 {'id': 10, 'name': 'white-knight', 'supercategory': 'pieces'},  
 {'id': 11, 'name': 'white-pawn', 'supercategory': 'pieces'},  
 {'id': 12, 'name': 'white-queen', 'supercategory': 'pieces'},  
 {'id': 13, 'name': 'white-rook', 'supercategory': 'pieces'}]
</code></pre><p>We have to alter the structure of these lists to create an efficient pipeline:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>new_anno_dict <span style=color:#f92672>=</span> {}
new_cat_dict <span style=color:#f92672>=</span> {}

<span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> cat_list:
    new_cat_dict[item[<span style=color:#e6db74>&#39;id&#39;</span>]] <span style=color:#f92672>=</span> item[<span style=color:#e6db74>&#39;name&#39;</span>]
    

<span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> anno_list:
    img_id <span style=color:#f92672>=</span> item<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;image_id&#39;</span>)
    <span style=color:#66d9ef>if</span> img_id <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> new_anno_dict:
        temp_list <span style=color:#f92672>=</span> []
        temp_list<span style=color:#f92672>.</span>append(item)
        new_anno_dict[img_id] <span style=color:#f92672>=</span> temp_list
    <span style=color:#66d9ef>else</span>:
        new_anno_dict<span style=color:#f92672>.</span>get(img_id)<span style=color:#f92672>.</span>append(item)
</code></pre></div><p>Now, let’s create a simple augmentation pipeline that flips our image horizontally and adds a parameter for bounding boxes:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>transform <span style=color:#f92672>=</span> A<span style=color:#f92672>.</span>Compose(
    [A<span style=color:#f92672>.</span>HorizontalFlip(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)],
    bbox_params<span style=color:#f92672>=</span>A<span style=color:#f92672>.</span>BboxParams(format<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;coco&#39;</span>, label_fields<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;category_ids&#39;</span>]),
)
</code></pre></div><p>Lastly, we will create a dataset similar to the <a href=https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset><strong>Dataset class</strong></a> offered by Pytorch. To do this, we need to define a class that implements the methods <code>__len__</code> and <code>__getitem__</code>.</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ImageDataset</span>:
    <span style=color:#66d9ef>def</span> __init__(self, path, img_list, anno_dict, cat_dict, albumentations<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
        self<span style=color:#f92672>.</span>path <span style=color:#f92672>=</span> path
        self<span style=color:#f92672>.</span>img_list <span style=color:#f92672>=</span> img_list
        self<span style=color:#f92672>.</span>anno_dict <span style=color:#f92672>=</span> anno_dict
        self<span style=color:#f92672>.</span>cat_dict <span style=color:#f92672>=</span> cat_dict
        self<span style=color:#f92672>.</span>albumentations <span style=color:#f92672>=</span> albumentations
    
    <span style=color:#66d9ef>def</span> __len__(self):
        <span style=color:#66d9ef>return</span> len(self<span style=color:#f92672>.</span>img_list)
    
    <span style=color:#66d9ef>def</span> __getitem__(self, idx):
        <span style=color:#75715e># Each image may have multiple objects thereby multiple bboxes</span>
        bboxes <span style=color:#f92672>=</span> [item[<span style=color:#e6db74>&#39;bbox&#39;</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>anno_dict[int(idx)]]
        cat_ids <span style=color:#f92672>=</span> [item[<span style=color:#e6db74>&#39;category_id&#39;</span>] <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>anno_dict[int(idx)]]
        categories <span style=color:#f92672>=</span> [self<span style=color:#f92672>.</span>cat_dict[idx] <span style=color:#66d9ef>for</span> idx <span style=color:#f92672>in</span> cat_ids]
        image <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>img_list[idx]
        img <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>self<span style=color:#f92672>.</span>path<span style=color:#e6db74>}{</span>image<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;file_name&#39;</span>)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
        img <span style=color:#f92672>=</span> img<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#34;RGB&#34;</span>)
        img <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(img)
        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>albumentations <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
            augmented <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>albumentations(image<span style=color:#f92672>=</span>img, bboxes<span style=color:#f92672>=</span>bboxes, category_ids<span style=color:#f92672>=</span>cat_ids)
            img <span style=color:#f92672>=</span> augmented[<span style=color:#e6db74>&#34;image&#34;</span>]
        <span style=color:#66d9ef>return</span> {
            <span style=color:#e6db74>&#34;image&#34;</span>: img,
            <span style=color:#e6db74>&#34;bboxes&#34;</span>: augmented[<span style=color:#e6db74>&#34;bboxes&#34;</span>],
            <span style=color:#e6db74>&#34;category_ids&#34;</span>: augmented[<span style=color:#e6db74>&#34;category_ids&#34;</span>],
            <span style=color:#e6db74>&#34;category&#34;</span>: categories
        }

 <span style=color:#75715e># path is the path to the json_file and images</span>
dataset <span style=color:#f92672>=</span> ImageDataset(path, image_list, new_anno_dict, new_cat_dict, transform)
</code></pre></div><p>Here are some of the results while iterating on the custom dataset:</p>
<p><img src=/posts_img/fast_feature_engineering/img_7.jpeg alt>
<img src=/posts_img/fast_feature_engineering/img_8.jpeg alt>
<img src=/posts_img/fast_feature_engineering/img_9.jpeg alt>
<img src=/posts_img/fast_feature_engineering/img_10.jpeg alt>
<img src=/posts_img/fast_feature_engineering/img_11.jpeg alt></p>
<p>Thus, we can now easily pass this custom dataset to a data loader to train our model.</p>
<hr>
<h3 id=feature-extraction>Feature Extraction</h3>
<p>You may have heard of pre-trained models being used to train image classifiers and for other supervised learning tasks.</p>
<p>But, did you know that you can also use pre-trained models for feature extraction of images?</p>
<p>In short feature extraction is a form of dimensionality reduction where a large number of pixels are reduced to a more efficient representation.</p>
<p>This is primarily useful for unsupervised machine learning tasks such as reverse image search.</p>
<p>Let’s try to extract features from images using Pytorch’s pre-trained models. To do this, we must first define our feature extractor class:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ResnetFeatureExtractor</span>(nn<span style=color:#f92672>.</span>Module):
    <span style=color:#66d9ef>def</span> __init__(self, model):
        super(ResnetFeatureExtractor, self)<span style=color:#f92672>.</span>__init__()
        self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(<span style=color:#f92672>*</span>model<span style=color:#f92672>.</span>children())[:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>model(x)
</code></pre></div><p>Note that in line 4, a new model is created with all of the layers of the original save for the last one. You will recall that the last layer in a neural network is a dense layer used for prediction outputs.</p>
<p>However, since we are only interested in extracting features, we do not require this last layer. Hence, it is excluded.</p>
<p>We then utilize torchvision’s pre-trained <code>resnet34</code> model by passing it to the <code>ResnetFeatureExtractor</code> constructor.</p>
<p>Let’s use the famous <a href=https://paperswithcode.com/dataset/cifar-10><strong>CIFAR10 dataset</strong></a> (50000 images), and loop over it to extract the features.</p>
<p><img src=/posts_img/fast_feature_engineering/img_12.png alt></p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>cifar_dataset <span style=color:#f92672>=</span> CIFAR10(<span style=color:#e6db74>&#34;./&#34;</span>, transform<span style=color:#f92672>=</span>transforms<span style=color:#f92672>.</span>ToTensor(), download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
cifar_dataloader <span style=color:#f92672>=</span> DataLoader(cifar_dataset, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)

feature_extractor<span style=color:#f92672>.</span>eval()
feature_list <span style=color:#f92672>=</span> []

<span style=color:#66d9ef>for</span> _, data <span style=color:#f92672>in</span> enumerate(tqdm_notebook(cifar_dataloader)):
    inputs, labels <span style=color:#f92672>=</span> data
    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
        extracted_features <span style=color:#f92672>=</span> feature_extractor(inputs)
    extracted_features <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>flatten(extracted_features)
    feature_list<span style=color:#f92672>.</span>append(extracted_features)
</code></pre></div><p>We now have a list of 50000 image feature vectors with each feature vector of size 512 (output size of the penultimate layer of the original resnet model).</p>
<pre tabindex=0><code>print(f&quot;Number of feature vectors: {len(feature_list)}&quot;) #50000  
print(f&quot;Number of feature vectors: {len(feature_list[0])}&quot;) #512
</code></pre><p>Thus, this list of feature vectors can now be used by statistical learning models such as KNN to search for similar images.</p>
<p>If you have reached this far then thank you very much for reading this article! I hope you have a fantastic day ahead! 😄</p>
<p><strong>👉</strong> <a href=https://github.com/Sayar1106/TowardsDataSciencecodefiles/tree/master/fast_feature_engineering><strong>Code used in the article</strong></a></p>
<p>Until next time! ✋</p>
<hr>
<h3 id=references>References:</h3>
<ul>
<li><a href=https://www.cs.toronto.edu/~kriz/cifar.html>https://www.cs.toronto.edu/~kriz/cifar.html</a></li>
<li><a href=https://www.practicaldeeplearning.ai/>https://www.practicaldeeplearning.ai/</a></li>
</ul>
</div>
<div class="media author-card" itemscope itemtype=http://schema.org/Person>
<div class=media-body>
<h5 class=card-title itemprop=name><a href=https://examplesite.org>Sayar Banerjee</a></h5>
<h6 class=card-subtitle>Analyst</h6>
<p class=card-text itemprop=description>My research interests include Deep Learning, NLP, Computer Vision</p>
<ul class=network-icon aria-hidden=true>
<li>
<a itemprop=sameAs href=/#contact>
<i class="fas fa-envelope"></i>
</a>
</li>
<li>
<a itemprop=sameAs href=https://twitter.com/sayar_banner target=_blank rel=noopener>
<i class="fab fa-twitter"></i>
</a>
</li>
<li>
<a itemprop=sameAs href=https://medium.com/@sayarbanerjee target=_blank rel=noopener>
<i class="fab fa-medium"></i>
</a>
</li>
<li>
<a itemprop=sameAs href=https://github.com/Sayar1106 target=_blank rel=noopener>
<i class="fab fa-github"></i>
</a>
</li>
<li>
<a itemprop=sameAs href=https://www.linkedin.com/in/sayarbanerjee/ target=_blank rel=noopener>
<i class="fab fa-linkedin"></i>
</a>
</li>
</ul>
</div>
</div>
</div>
</article>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script>
<script>hljs.initHighlightingOnLoad()</script>
<script src=/js/academia.min.e40e230d9b3dfeac86994156b6388764.js></script>
<div class=container>
<footer class=site-footer>
<div class=container>
<div class="row align-items-center">
<div class="col-md-6 mb-4 mb-md-0">
<p class=mb-0>
Copyright © 2021 &#183;
Powered by
<a href=https://gethugothemes.com target=_blank rel=noopener>Gethugothemes</a>
</p>
</div>
<div class=col-md-6>
<ul class="list-inline network-icon text-right mb-0">
</ul>
</div>
</div>
</div>
</footer>
</div>
<div id=modal class="modal fade" role=dialog>
<div class=modal-dialog>
<div class=modal-content>
<div class=modal-header>
<h5 class=modal-title>Cite</h5>
<button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span>
</button>
</div>
<div class=modal-body>
<pre><code class="tex hljs"></code></pre>
</div>
<div class=modal-footer>
<a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank>
<i class="fas fa-copy"></i> Copy
</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank>
<i class="fas fa-download"></i> Download
</a>
<div id=modal-error></div>
</div>
</div>
</div>
</div>
</body>
</html>